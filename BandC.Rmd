---
title: "TMA4300 Project 3"
author: "Martin"
date: "March 25, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
```

## B
In this task we have labeled measured data of three different individuals. A box plot of the data is shown in figure \ref{fig:bili}.

```{r, include=FALSE, fig.cap="\\label{fig:bili} Box plot of bilirubin data. At first glance, it looks like person 3 has a higher bilirubin concentration than the two others."}
bilirubin <- read.table("bilirubin.txt",header=T)
boxplot(log(meas)~pers, data = bilirubin, main = "Logratihm of concentrations")
```


We want to fit a linear model
$$ \log Y_{ij} = \beta_i + \varepsilon_{ij}$$,
and test
\begin{equation}
  H_0: \beta_0 = \beta_1 = \beta_2, \quad H_1: \text{At least one } \beta_i \text{   different from the others}.
\end{equation}
Under the null hypothesis, the data should be iid, meaning that labels should not matter. This is the idea of a permutation test. If the null hypothesis is correct, changing labels should not matter.

In the permutation test, we create a permuted dataset $\mathbf{x}^\ast$, from our original dataset $\mathbf{x}$, and calculate our test statistic $t(\mathbf{x}^\ast)$. We do this $B$ times, and the p-value can then be approximated by $\frac{1}{B}$ times the number of statistics satisfying $t(\mathbf{x}^\ast) \ge t(\mathbf{x})$. In other words, we approximate the probability of obtaining a test statistic ast least as extreme as the "original" test statistic $t(\mathbf{x})$.

```{r, fig.cap="Box plot of bilirubin data"}
set.seed(314)

linearmodel = lm(log(meas)~pers, data = bilirubin)
Fval = summary(linearmodel)$fstatistic[1] # Value of F-statistic
Fval
n1 = 11; n2 = 10; n3 = 8; n = n1 + n2 + n3
permTest <- function(){
  permbilirubin = copy(bilirubin)
  permbilirubin$meas = sample(bilirubin$meas,n, replace = F)
  linearmodel = lm(log(meas)~pers, data = permbilirubin)
  return(summary(linearmodel)$fstatistic[1])
}
```
Fitting the linear regression on the original dataset gives a p-value of $0.03946$, rather significant. Results from running the permutation test with $B=999$ is shown in figure \ref{fig:perm}. re more useful when trying to obtain a p-value when you do not know the distribution.


```{r, include=FALSE, fig.cap="\\label{fig:perm} Result from running the permutation test"}
B = 999
results = replicate(B,permTest())
hist(results, freq=F, breaks=50)
x = (0:1000)/12
pdf = sapply(x,df,2,26)
lines(x,pdf)
abline(v = Fval, col = "red")
newp = length(which(results >= Fval))/B
"Permutation test p-value:"
newp
abline(v = qf(1-newp, 2,26), col= "blue")
legend("topright", legend= c("Original F value", "Permutation test F value"), col=c("red","blue"), lty=c(1,1))
```

We see that the statistics obtained by permutation nicely fit the distribution they come from. Furthermore, the new p-value is very similar to the old one, and approaches the original one nicely for large $B$. In this case, we know the distribution under the null hypothesis, and therefore the correct p-value. Permutation tests are more useful to approximate p-values when we do not know the null hypothesis distribution. Here we have essentially shown that we can use a permutation test in this case, and it nicely approximates the p-value it is supposed to approximate.

## C.1
$x$ and $y$ are exponentially distributed, and they are dependent. We can therefore write the likelihood:
\begin{align*}
  f(x,y|\lambda_0,\lambda_1) = f(x|\lambda_0)f(y|\lambda_1) = \prod_i^n p(x_i|\lambda_0) \prod_i^n p(y_i|\lambda_1)\\
  \log f(x,y|\lambda_0,\lambda_1) = \sum_i^n \log p(x_i|\lambda_0) + \sum_i^n \log p(y_i|\lambda_1) \\
  = \sum_i^n \log \lambda_0 \exp(-\lambda_0 x_i) + \sum_i^n \log \lambda_1 \exp(-\lambda_1 y_i) = n(\log \lambda_0 + \log \lambda_1) - \lambda_0 \sum_i^n x_i - \lambda_1 \sum_i^n y_i \\
  E(\log f(x,y|\lambda_0,\lambda_1)|z,u,\lambda_0^{(t)},\lambda_1^{(t)}) = n(\log \lambda_0 + \log \lambda_1) - \lambda_0 \sum_i^n E(x_i|z_i,u_i,\lambda_0^{(t)},\lambda_1^{(t)}) - \lambda_1 \sum_i^n E(y_i|z_i,u_i,\lambda_0^{(t)},\lambda_1^{(t)})
\end{align*}
Need $E(x_i|z_i,u_i,\lambda_0^{(t)}, \lambda_1^{(t)})$ and $E(y_i|z_i,u_i,\lambda_0^{(t)}, \lambda_1^{(t)})$.
Know that (don't think we need this)
\begin{align*}
  p(z \le v) = p(\max\{x,y\} \le v) = p(x \le v) p(y \le v) = (1 - \exp(-\lambda_0 v))(1 - \exp(-\lambda_1 v)) \\
  p(z) = \frac{d}{dv} p(z \le v) = \lambda_0 \exp(-\lambda_0 z)(1- \exp(-\lambda_1 z)) + \lambda_1 \exp(-\lambda_1 z)(1- \exp(-\lambda_0 z)).
\end{align*}

Can find
\begin{align*}
  E(x_i|z_i,u_i,\lambda_0^{(t)}, \lambda_1^{(t)}) = \begin{cases} z_i, \quad &\text{if } u_i = 1 \\ E(x_i|z_i,u_i = 0,\lambda_0^{(t)}, \lambda_1^{(t)})  \quad &\text{if } u_i = 0 \end{cases}.
\end{align*}
We can calculate
\begin{align*}
E(x_i|z_i,u_i = 0,\lambda_0^{(t)}, \lambda_1^{(t)}) &= E(x_i|x_i < y_i) \\
&= \left(\int_0^{y_i} x_i \lambda_0^{(t)} \exp(-\lambda_0^{(t)}x_i)\right)\big/\left(\int_0^{y_i} x_i \lambda_0^{(t)} \exp(-\lambda_0^{(t)}x_i)\right) \\
&= \frac{1- \exp(-\lambda_0^{(t)}y_i)(\lambda_0^{(t)}y_i + 1)}{\lambda_0^{(t)}(1- \exp(-\lambda_0^{(t)}y_i))}\\
&= \frac{\exp(\lambda_0^{(t)}y_i) - (\lambda_0^{(t)}y_i + 1)}{\lambda_0^{(t)}(\exp(\lambda_0^{(t)}y_i) - 1)} \\
&= \frac{1}{\lambda_0^{(t)}} - \frac{y_i}{\exp(\lambda_0^{(t)}y_i) - 1)} \\
&= \frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp(\lambda_0^{(t)}z_i) - 1)}.
\end{align*}
In the first step we calculate the weighted mass of our pdf for $x_i < y_i$, and we remember to divide by the mass of this area, in order to obtain the correct expectation. Furthermore, because $u_i = 0$, we know that $y_i = z_i$. We can use the same steps to calculate $E(y_i|z_i,u_i,\lambda_0^{(t)}, \lambda_1^{(t)})$, and we can write the expectations more compactly as
\begin{align*}
E(x_i|z_i,u_i,\lambda_0^{(t)}, \lambda_1^{(t)}) &= u_iz_i +(1-u_i)\left(\frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp(\lambda_0^{(t)}z_i) - 1)}\right) \\
E(y_i|z_i,u_i,\lambda_0^{(t)}, \lambda_1^{(t)}) &= (1-u_i)z_i +u_i\left(\frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp(\lambda_1^{(t)}z_i) - 1)}\right).
\end{align*}
Thus, we can write the expectation of the log-likelihood as
\begin{align*}
E(\log f(x,y|\lambda_0,\lambda_1)|z,u,\lambda_0^{(t)},\lambda_1^{(t)}) = n(\log \lambda_0 + \log \lambda_1) &-\lambda_0 \sum_i^n u_iz_i +(1-u_i)\left(\frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp(\lambda_0^{(t)}z_i) - 1)}\right)\\
&- \lambda_1 \sum_i^n (1-u_i)z_i +u_i\left(\frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp(\lambda_1^{(t)}z_i) - 1)}\right))
\end{align*}

## C.2

Now that we have the loglikelihood we can use the EM-algorithm. The method is simple:
\begin{itemize}
\item Choose initial values $(\lambda_0^{(0)},\lambda_1^{(0)})$
\item Maximize $E(\log f(x,y|\lambda_0,\lambda_1)|z,u,\lambda_0^{(t)},\lambda_1^{(t)})$ wrt to $(\lambda_0,\lambda_1)$, and choose this as $(\lambda_0^{(t+1)},\lambda_1^{(t+1)})$
\item Iterate until convergence
\end{itemize}
The maximum can be found analytically by using the first order neccessary condition, that is, the gradient is zero (FIXME)
\begin{align*}
\nabla E(\log f(x,y|\lambda_0,\lambda_1)|z,u,\lambda_0^{(t)},\lambda_1^{(t)}) = 0 \\
\implies \frac{n}{\lambda_0} - \sum_i^n u_iz_i +(1-u_i)\left(\frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp(\lambda_0^{(t)}z_i) - 1)}\right) &= 0 \\
\frac{n}{\lambda_1} - \sum_i^n u_iz_i +(1-u_i)\left(\frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp(\lambda_1^{(t)}z_i) - 1)}\right) &= 0 \\
\implies \lambda_0^{(t+1)} = \frac{n}{\sum_i^n u_iz_i +(1-u_i)\left(\frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp(\lambda_0^{(t)}z_i) - 1)}\right)} \\
\lambda_1^{(t+1)} = \frac{n}{\sum_i^n u_iz_i +(1-u_i)\left(\frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp(\lambda_1^{(t)}z_i) - 1)}\right)}.
\end{align*}
Furthermore, we can use the second order sufficient condition, that is, the hessian is negative definite, to prove that this is indeed the maximal value
\begin{align*}
\nabla^2 E(\log f(x,y|\lambda_0,\lambda_1)|z,u,\lambda_0^{(t)},\lambda_1^{(t)}) = n \begin{bmatrix} -\frac{1}{\lambda_0} & 0 \\ 0 & -\frac{1}{\lambda_1} \end{bmatrix},
\end{align*}
which is obviously negative definite for positive $\lambda_0$ and $\lambda_1$, and we have found the maximizers of $E(\log f(x,y|\lambda_0,\lambda_1)|z,u,\lambda_0^{(t)},\lambda_1^{(t)})$.

```{r}
z = read.table('z.txt', header = FALSE, sep = "", dec = ".")
u = read.table('u.txt', header = FALSE, sep = "", dec = ".")
n = length(z[,])
lambda_0_next <- function(lambda_0_prev, u, z){
  sum = 0
  for (i in 1:n){
    if (u[i] == 1){sum = sum + z[i]}
    else {sum = sum + 1.0/lambda_0_prev - z[i]/(exp(lambda_0_prev * z[i])-1)}
  }
  return(n/sum)
}
lambda_1_next <- function(lambda_1_prev, u, z){
  sum = 0
  for (i in 1:n){
    if (u[i] == 0){sum = sum + z[i]}
    else {sum = sum + 1.0/lambda_1_prev - z[i]/(exp(lambda_1_prev * z[i])-1)}
  }
  return(n/sum)
}
EM <- function(u, z, lambda_0_init = 1.0, lambda_1_init = 1.0, plot=0){
  MAXITER = 50
  k = MAXITER
  TOL = 1e-6
  lambda_0_list = numeric(MAXITER)
  lambda_1_list = numeric(MAXITER)
  lambda_0_list[1] = lambda_0_init
  lambda_1_list[1] = lambda_1_init
  for (j in 1:(MAXITER-1)){
      lambda_0_list[j+1] = lambda_0_next(lambda_0_list[j], u, z)
      lambda_1_list[j+1] = lambda_1_next(lambda_1_list[j], u, z)
      if(sqrt((lambda_0_list[j+1]-lambda_0_list[j])^2 + (lambda_1_list[j+1]-lambda_1_list[j])^2) < TOL){
        k = j
        break
      }
  }
  if(plot){
    plot(lambda_1_list[1:k], type="b", main="", xlab = "Iterations", ylab="", col="red")
    lines(lambda_0_list[1:k], type="b", col="blue")
  }
  return(list("lambda_0" = lambda_0_list[k], "lambda_1" = lambda_1_list[k]))
}
EM_result = EM(u[,],z[,],1)
true_lambda_0 = EM_result$lambda_0
true_lambda_1 = EM_result$lambda_1
```

## C.3
The EM-algorithm gives us estimates $(\hat{\lambda}_0, \hat{\lambda}_1)$, but says nothing about the standard deviations, bias or correlation of our estimators. Bootstrapping comes to the rescue. The algorithm can be written simply as
\begin{itemize}
\item For $b=1...B$ bootstrap samples
\item   sample $u^b$ and corresponding $z^b$ from the original dataset
\item   use EM algorithm to find $\hat{\lambda}_0^b$ and $\hat{\lambda}_1^b$
\item   optional: Use $\hat{\lambda}_0^{b-1}$ and $\hat{\lambda}_1^{b-1}$ as initial values in the EM algorithm for (hopefully) faster convergence
\item extract some interesting feature (mean, standard deviation and correlation) from the new populations $(\hat{\lambda}_0^\ast, \hat{\lambda}_1^\ast) = (\hat{\lambda}_0^1,...\hat{\lambda}_0^B, \hat{\lambda}_1^1,...\hat{\lambda}_1^B)$.
\end{itemize}

```{r}
B = 1000 # Number of bootstrap samples
lambda_0_strapped = numeric(B)
lambda_1_strapped = numeric(B)
lambda_0_init = 1.0
lambda_1_init = 1.0
for (i in 1:B){
  strapped_indices = sample(1:n, n, replace=T)
  EM_result = EM(u[strapped_indices,], z[strapped_indices,], lambda_0_init, lambda_1_init, 0)
  lambda_0_strapped[i] = EM_result$lambda_0
  lambda_1_strapped[i] = EM_result$lambda_1
  lambda_0_init = EM_result$lambda_0
  lambda_1_init = EM_result$lambda_1
}
"sd of lambda_0"
sd(lambda_0_strapped)
"sd of lambda_1"
sd(lambda_1_strapped)
"bias of lambda_0"
mean(lambda_0_strapped) - true_lambda_0
"bias of lambda_1"
mean(lambda_1_strapped) - true_lambda_1
"correlation between lambda_0 and lambda_1"
cor(lambda_0_strapped, lambda_1_strapped)
```

## C.4
Now we try to find the likelihood $f_{Z_i,U_i}(z_i,u_i|\lambda_0,\lambda_1)$ analytically, which we can use to find the maximum likelihood estimators $\hat{\lambda}_0$ and $\hat{\lambda}_1$.